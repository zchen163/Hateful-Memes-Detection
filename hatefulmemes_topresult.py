# -*- coding: utf-8 -*-
"""hatefulmemes_topresult.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KoC1216r2JjNppq7Epj9vWvOFYOPVU6c

# <font color='Aqua'> <b> Team HateDetectron - Phase-2 Submissions </b> </font>

---
This notebook is only for reproducing the results of Phase-2 submissions by the team `HateDetectron`. In other words, just loading the final models and getting predictions for the test set. See the [end2end-process notebook](https://colab.research.google.com/drive/1O0m0j9_NBInzdo3K04jD19IyOhBR1I8i?usp=sharing) to see the whole approach in detail: how the models are trained, how the image features are extracted, which datasets are used, etc.

---
**Author:**\
<font color='Wheat'>
    <b>
        Riza Velioglu
    </b>
</font>

**Contact:**

<center>
<a href="http://rizavelioglu.github.io/"><img src="https://drive.google.com/uc?id=1SWc-ryZf7xxZ_g7AdU_vn2Y451IcCisw" width="300"></a>

[Webpage](http://rizavelioglu.github.io/)
</center>
"""

#@markdown ---
#@title <h1><b> <font color='lightblue'> Running the whole notebook at once! </font> <font color='red'> --Action required!-- </b></font></h1> { run: "auto" }
#@markdown First, please specify the download link and the `.zip` password which both can be taken from [DrivenData](https://www.drivendata.org/competitions/70/hateful-memes-phase-2/data/)


YOUR_LINK_TO_DOWNLOAD_PHASE2_DATA = '' #@param {type:"string"}
PASSWORD_OF_ZIP = '' #@param {type:"string"}

#@markdown Then run all the cells:
#@markdown - <font color='yellow'> Colab </font>: **"Runtime" > "Run All"** *OR* `Ctrl+F9`
#@markdown - <font color='orange'> Jupyter Notebook </font>: **"Cell" > "Run All"**
#@markdown ---

"""# Table of Contents

<details><summary>
<font color='Tan'> I. Installation of MMF & dependencies </font></summary>

- install MMF dependencies
- install MMF from source
</details>

<details><summary>
<font color='Tan'> II. Download the dataset & convert it into MMF format </font></summary>

- download Hateful Memes (HM) dataset
- convert HM into MMF format (unzip and place the dataset to a specific location)
- remove unnecessary data to keep the disk clean
</details>


<details><summary>
<font color='Tan'> III. Feature Extraction </font></summary>

- download image features (.zip files) for 2 datasets:  HM and [Memotion](https://www.kaggle.com/williamscott701/memotion-dataset-7k) [( see paper )](https://arxiv.org/pdf/2008.03781.pdf)
- extract the two `.zip` files and merge them into one folder
</details>

<details><summary>
<font color='Tan'> IV. Validating'fine-tuned' VisualBERT models on `dev_unseen.jsonl` </font></summary>

- download fine-tuned models that were used in Phase-2 submission
- evaluate them on 'dev_unseen' data
</details>


<details><summary>
<font color='Tan'> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </font></summary>

- generate predictions for 2 submissions in Phase-2
</details>

## <font color='magenta'> <b> I. Installation of MMF & dependencies </b> </font>

Please set your `$HOME` directory.\
**e.g.** For <font color='orange'> Linux </font> users it can be: `"/home/project_folder"`,\
For <font color='yellow'> Colab </font> it would be: `"/content"`
"""

import os
home = "/content"
os.chdir(home)
os.getcwd()

# Install specified versions of `torch` and `torchvision`, before installing mmf (causes an issue)
!pip install torch==1.6.0 torchvision==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html

"""#### *Install MMF from source* 

"""

# Clone the following repo where mmf does not install default image features, 
# since we will use our own features
!git clone --branch no_feats --config core.symlinks=true https://github.com/rizavelioglu/mmf.git

import os
os.chdir(os.path.join(home, "mmf"))

!pip install --editable .

"""---
## <font color='magenta'> <b> II. Download the dataset & convert it into *MMF* format </b> </font>

### Phase 2 data
"""

!wget -O XjiOc5ycDBRRNwbhRlgH.zip --no-check-certificate --no-proxy "$YOUR_LINK_TO_DOWNLOAD_PHASE2_DATA"

!mmf_convert_hm --zip_file="XjiOc5ycDBRRNwbhRlgH.zip" --password=$PASSWORD_OF_ZIP --bypass_checksum 1

# Free up the disk by removing .zip, .tar files
!rm -rf /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/XjiOc5ycDBRRNwbhRlgH.zip
!rm -rf $home/mmf/XjiOc5ycDBRRNwbhRlgH.zip

"""---
## <font color='magenta'> <b> III. Feature Extraction </b> </font>

### <font color='lightgreen'> <b> Collect 'pre-extracted' features </b> </font>

There are 2 .zip files which stores the extracted image features: one for <font color='Salmon'>Hateful Memes</font>, and one for <font color='Salmon'> Memotion Dataset</font>.

The following cell downloads both .zip files into the `$HOME` directory:
- [hateful_memes_features](https://drive.google.com/file/d/1YGigTCQQlVvS726YuECTMx0He8p0Xle2/view?usp=sharing)
- [memotion_features](https://drive.google.com/file/d/11o35vKEMDQjvHV42aYMzwjEVlIDZIe1a/view?usp=sharing)
"""

os.chdir(home)
# download HM dataset features
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YGigTCQQlVvS726YuECTMx0He8p0Xle2' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1YGigTCQQlVvS726YuECTMx0He8p0Xle2" -O 'feats_hm.zip' && rm -rf /tmp/cookies.txt
# download Memotion dataset features
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11o35vKEMDQjvHV42aYMzwjEVlIDZIe1a' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=11o35vKEMDQjvHV42aYMzwjEVlIDZIe1a" -O 'feats_memotion.zip' && rm -rf /tmp/cookies.txt

# Unzip features for Memotion dataset & remove it to free disk
!unzip $home/feats_memotion.zip -d $home/features/
!rm -rf $home/feats_memotion.zip
# Unzip features for HM dataset & remove it to free disk
!unzip $home/feats_hm.zip -d $home/features/
!rm -rf $home/feats_hm.zip
# Move Memotion features into the same folder as HM features
!mv $home/features/feats_memotion/*.npy $home/features/feats_hm/

"""---
## <font color='magenta'> <b> IV. Validating'fine-tuned' VisualBERT models on `dev_unseen.jsonl`</b> </font>

### <font color='Violet'> <b> Submission#1 </b> </font>

|            | ROC-AUC | Accuracy |    Dataset   |
|------------|:-------:|:--------:|:------------:|
|Submission#1| $0.7555$| $0.7352$ | `dev_unseen` |
|Submission#1| $0.8108$| $0.7650$ | `test_unseen`|

The following cell downloads the fine-tuned model from [this link](https://drive.google.com/file/d/1NOX2lJkbK7sKRsg4_y_KUcLamknowsu2/view?usp=sharing) to the `$HOME` directory:
"""

"""
Uncomment it if needed
"""

# os.chdir(home)
# # Download the fine-tuned model
# !wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1NOX2lJkbK7sKRsg4_y_KUcLamknowsu2' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1NOX2lJkbK7sKRsg4_y_KUcLamknowsu2" -O 'submission#1.zip' && rm -rf /tmp/cookies.txt
# # unzip the model
# !unzip -qq $home/submission#1.zip -d $home/submission#1
# # remove the .zip after unzipping to free the disk
# !rm -rf $home/submission#1.zip

"""
Uncomment it if needed
"""

# # Validate the model on the dev_unseen data
# os.chdir(home)
# # where checkpoint is
# ckpt_dir = os.path.join(home, "submission#1/best.ckpt")
# feats_dir = os.path.join(home, "features/feats_hm")

# !mmf_run config="projects/visual_bert/configs/hateful_memes/defaults.yaml" \
#     model="visual_bert" \
#     dataset=hateful_memes \
#     run_type=val \
#     checkpoint.resume_file=$ckpt_dir \
#     checkpoint.reset.optimizer=True \
#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
#     dataset_config.hateful_memes.features.train[0]=$feats_dir \
#     dataset_config.hateful_memes.features.val[0]=$feats_dir \
#     dataset_config.hateful_memes.features.test[0]=$feats_dir \

"""### <font color='Violet'> <b> Submission#2 </b> </font>

|            | ROC-AUC | Accuracy |    Dataset   |
|------------|:-------:|:--------:|:------------:|
|Submission#2| $0.7757$| $0.7315$ | `dev_unseen` |
|Submission#2| $0.8268$| $0.7805$ | `test_unseen`|

The following cell downloads the fine-tuned model from [this link](https://drive.google.com/file/d/1To4L0on-Us-DHFn53b21lYYcl6RgaDCB/view?usp=sharing) to the `$HOME` directory:
"""

"""
Uncomment it if needed
"""

# os.chdir(home)
# # Download the fine-tuned model
# !wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1To4L0on-Us-DHFn53b21lYYcl6RgaDCB' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1To4L0on-Us-DHFn53b21lYYcl6RgaDCB" -O 'submission#2.zip' && rm -rf /tmp/cookies.txt
# # Unzip the model
# !unzip -q $home/submission#2.zip -d $home/submission#2
# # remove the .zip after unzipping to free the disk
# !rm -rf $home/submission#2.zip

"""
Uncomment it if needed
"""

# os.chdir(home)
# # where checkpoint is
# ckpt_dir = os.path.join(home, "submission#2/best.ckpt")
# feats_dir = os.path.join(home, "features/feats_hm")

# !mmf_run config="projects/visual_bert/configs/hateful_memes/defaults.yaml" \
#     model="visual_bert" \
#     dataset=hateful_memes \
#     run_type=val \
#     checkpoint.resume_file=$ckpt_dir \
#     checkpoint.reset.optimizer=True \
#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
#     dataset_config.hateful_memes.features.train[0]=$feats_dir \
#     dataset_config.hateful_memes.features.val[0]=$feats_dir \
#     dataset_config.hateful_memes.features.test[0]=$feats_dir \

"""### <font color='Violet'> <b> Submission#3 ($Best\ Submission$)</b> </font>

|            | ROC-AUC | Accuracy |    Dataset   |
|------------|:-------:|:--------:|:------------:|
|Submission#3| $-$     | $-$      | `dev_unseen` |
|Submission#3| $0.8518$| $0.8050$ | `test_unseen`|

After a hyper-parameter search, we ended up having multiple models having different ROC-AUC scores on `dev_unseen` dataset. We sorted them by the ROC score and took all the models that have a ROC score of `0.76` or higher (the threshold is chosen arbitrarily). The following figure shows all the $27$ models and its ROC-scores, as well as its hyper-parameters ([see this document for all the model scores, 60+ models in total](https://docs.google.com/spreadsheets/d/11m2p7vNxHhZWumkFNvv6d94HcqG77DItRX7MuGfOtWA/edit?usp=sharing)).

<details><summary>
<font color='Tan'> Figure 1: ROC-AUC scores (on `dev_unseen`) of different VisualBERT models </font></summary>

<img src="https://drive.google.com/uc?id=10WUBnSO5L5O44c8WCxHRA_iFZudH73lF" width="1000"> 

</details>

Then, predictions are collected from each of the $27$ models and the `Majority Voting` technique is applied: the `class` of a data point is determined by the majority voted class.

> This technique is also known as: [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html), [Ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning), [Bootstrap Aggregating (BAGGING)](https://en.wikipedia.org/wiki/Bootstrap_aggregating#:~:text=Bootstrap%20aggregating%2C%20also%20called%20bagging,and%20helps%20to%20avoid%20overfitting.)

Please see [this document](https://drive.google.com/file/d/1vjUsMqaqjZdoNj0w989RX7GKkPod-CGo/view?usp=sharing) to see all the model predictions for `test_unseen` and how the technique is applied.

**Note:**  The `proba` column in the submission which stands for the probability of a data point belonging to a class is chosen to be the maximum probability among all of the $27$ models if $Class\ 1$, and the minumum if $Class\ 0$.

See the Part<font color='magenta'> <b> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </b> </font> for the code!

---
## <font color='magenta'> <b> V. Generate predictions for the Challenge (`test_unseen.jsonl`) </b> </font>

### <font color='Thistle'> <b> Submission#1 </b> </font>
"""

"""
Uncomment it if needed
"""

# os.chdir(home)
# # where checkpoint is
# ckpt_dir = os.path.join(home, "submission#1/best.ckpt")
# feats_dir = os.path.join(home, "features/feats_hm")

# !mmf_predict config="projects/visual_bert/configs/hateful_memes/defaults.yaml" \
#     model="visual_bert" \
#     dataset=hateful_memes \
#     run_type=test \
#     checkpoint.resume_file=$ckpt_dir \
#     checkpoint.reset.optimizer=True \
#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
#     dataset_config.hateful_memes.features.train[0]=$feats_dir \
#     dataset_config.hateful_memes.features.val[0]=$feats_dir \
#     dataset_config.hateful_memes.features.test[0]=$feats_dir \

"""### <font color='Thistle'> <b> Submission#2 </b> </font>"""

"""
Uncomment it if needed
"""

# os.chdir(home)
# # where checkpoint is
# ckpt_dir = os.path.join(home, "submission#2/best.ckpt")
# feats_dir = os.path.join(home, "features/feats_hm")

# !mmf_predict config="projects/visual_bert/configs/hateful_memes/defaults.yaml" \
#     model="visual_bert" \
#     dataset=hateful_memes \
#     run_type=test \
#     checkpoint.resume_file=$ckpt_dir \
#     checkpoint.reset.optimizer=True \
#     dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_unseen.jsonl \
#     dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_unseen.jsonl \
#     dataset_config.hateful_memes.features.train[0]=$feats_dir \
#     dataset_config.hateful_memes.features.val[0]=$feats_dir \
#     dataset_config.hateful_memes.features.test[0]=$feats_dir \

"""### <font color='Thistle'> <b> Submission#3  ($Best\ Submission$)</b> </font>

"""

os.mkdir(f"{home}/sub3/")
os.chdir(os.path.join(home, "sub3"))
!git clone https://github.com/rizavelioglu/hateful_memes-hate_detectron.git
!cp hateful_memes-hate_detectron/utils/generate_submission.sh .
!chmod +x generate_submission.sh

"""Download 27 models in a `.7z` file and extract them all"""

# Download the .7z file which includes all the 27 models
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1D1nehiowEHMxJwijybfuTiC1835wZaHk' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1D1nehiowEHMxJwijybfuTiC1835wZaHk" -O 'majority_voting_models.7z' && rm -rf /tmp/cookies.txt
# Extract the .7z file to get the models
# !p7zip -d 'majority_voting_models.7z'
# !sudo apt-get install p7zip-full p7zip-rar
!7z e 'majority_voting_models.7z'

"""Generate submission for each model"""

from subprocess import call

models = [i for i in os.listdir(".") if i.endswith(".ckpt")]

print(f"[INFO] Getting predictions for {len(models)} models! This might take long..")
for model in models:
    feats_dir = os.path.join(home, "features/feats_hm")
    # Execute the bash script which gets predictions for 'test_unseen' data
    rc = call(f"./generate_submission.sh {model} {feats_dir}", shell=True)

"""Apply majority voting"""

import numpy as np
import pandas as pd

# Store all the prediction folders
folders = [i for i in os.listdir("save/preds") if i.startswith("hateful_memes")]
preds = pd.DataFrame()

try:
    for folder in folders:
        pred = [i for i in os.listdir(f"save/preds/{folder}/reports/") if i.endswith(".csv")]
        pred = pd.read_csv(f"save/preds/{folder}/reports/{pred[0]}")
        preds = pd.concat([preds, pred], axis=1)
except:
    pass

# assert len(preds.columns) == 27*3

# Create 
submission = pred
np_df = np.asarray(preds)

for idx, row in enumerate(np_df[:,:]):
    probas = row[1::3]
    labels = row[2::3]

    if sum(labels) > 13:
        submission.loc[idx, 'label']=1
        submission.loc[idx, 'proba']=probas.max()    
    else:
        submission.loc[idx, 'label']=0
        submission.loc[idx, 'proba']=probas.min()

"""Sort the submission with regards to the submission template & save the final submission file to `submission#3.csv`"""

# Download the Phase2 submission template
!wget -O submission_format_phase_2.csv  "https://drivendata-prod.s3.amazonaws.com/data/70/public/submission_format_phase_2.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCYVI2LMPSY%2F20201201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201201T023533Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=04330cf22c33f1817cac29509178d2c11a07d620e8237241c5088f4fd25df2b3"
os.chdir(os.path.join(home, "sub3"))
template = pd.read_csv("submission_format_phase_2.csv")
# Sort the 'submission' file
submission = submission.set_index('id')
submission = submission.reindex(index=template['id'])
submission = submission.reset_index()
# Save submission file
submission.to_csv(f"{home}/submission#3.csv", index=False)